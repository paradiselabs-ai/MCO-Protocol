// MCO Success Criteria - Persistent Memory and Evaluation Framework
// Building on mco.core context: Progressive revelation + iterative orchestration principles

@goal "Create a practical Smart Code Review Agent that demonstrates MCO's progressive revelation structure and iterative orchestration loops"
> Building on mco.core workflow context: The agent you create must be simple enough to understand but powerful enough to demonstrate MCO's capabilities.
> Building on educational purpose: Success means proving that MCO's orchestration makes basic agents incredibly reliable and effective.
> This goal will be evaluated through MCO's "try again, but better" loop until genuinely achieved.

@success_criteria_1 "Generated mco.core follows progressive revelation structure with each section building context from previous sections"
> Building on SNLP structure explanation from mco.core: Each @section and > block must reference and expand on previous information.
> MCO will evaluate: Does the workflow introduction flow naturally into description, then data, then agents, with consistent context building?
> Evaluation loop: If context doesn't build progressively, MCO responds "Try again, but better: Your sections don't connect contextually."

@success_criteria_2 "Code review agent workflow includes exactly 6 practical steps with MCO evaluation points after each step"
> Building on target_agent_steps from mco.core: The 6 steps must be manageable yet demonstrate real code review capability.
> MCO will evaluate: Are the steps clear, focused, and designed for iterative improvement through orchestration loops?
> Evaluation loop: If steps lack evaluation points, MCO responds "Try again, but better: Add MCO evaluation after each step."

@success_criteria_3 "MCP server integration follows simple, reliable patterns without unnecessary complexity"
> Building on mcp_integration_requirements from mco.core: JSON-RPC over stdio, graceful error handling, retry logic.
> MCO will evaluate: Is the integration practical and focused on demonstrating orchestration rather than technical complexity?
> Evaluation loop: If integration is overcomplicated, MCO responds "Try again, but better: Simplify the MCP connection pattern."

@success_criteria_4 "Generated SNLP files demonstrate both progressive revelation AND iterative orchestration concepts clearly"
> Building on MCO's breakthrough innovation from mco.core: The dual power of progressive context + improvement loops.
> MCO will evaluate: Do the generated files show how context builds progressively while enabling iterative improvement?
> Evaluation loop: If concepts aren't clear, MCO responds "Try again, but better: Make the progressive revelation + orchestration loop patterns explicit."

@success_criteria_5 "Complete workflow enables autonomous code review that improves through MCO's 'try again, but better' mechanism"
> Building on persistent improvement philosophy from mco.core: The agent must get better at code review through orchestration.
> MCO will evaluate: Would this workflow actually produce reliable, improving code review results when deployed?
> Evaluation loop: If autonomy isn't achievable, MCO responds "Try again, but better: Add more explicit improvement mechanisms."

@target_audience "AI developers learning MCO orchestration and autonomous agent design"
> Building on educational_context from mco.core: The audience is learning by experiencing MCO orchestration firsthand.
> These developers need to see how simple, well-orchestrated agents outperform complex, unorchestrated ones.
> The code review agent serves as a practical example they can understand and adapt.

@developer_vision "Autonomous agents that improve through orchestrated iteration rather than complex initial design"
> Building on target_audience context: Shift from "build it perfect" to "build it improvable through orchestration."
> MCO's vision: Simple agents + smart orchestration = reliable autonomous systems.
> This code review agent proves that orchestration sophistication trumps agent complexity.

// Evaluation Framework - MCO's Iterative Improvement Engine
@evaluation:
  frequency: "after_each_step_completion"
  > Building on developer_vision: Every step gets evaluated - no work proceeds without meeting quality standards.
  
  method: "progressive_context_check_plus_improvement_loop_validation"
  > Building on evaluation frequency: Check both context building AND orchestration loop effectiveness.
  
  quality_threshold: "all_criteria_must_genuinely_pass_mco_evaluation"
  > Building on evaluation method: MCO doesn't accept "good enough" - criteria must be truly satisfied.
  
  failure_response: "try_again_but_better_with_specific_contextual_feedback"
  > Building on quality threshold: When evaluation fails, MCO provides precise guidance for improvement.
  
  success_progression: "context_builds_naturally_and_orchestration_loops_function_effectively"
  > Building on failure response: Success means both progressive revelation AND iterative orchestration work together.

// Meta-Evaluation: How MCO Will Evaluate YOUR Creation Process
@meta_evaluation:
  your_experience_check: "Are you noticing how each section of this file builds on previous sections?"
  > This IS progressive revelation in action - you're experiencing the structure you need to create.
  
  orchestration_awareness: "When MCO evaluates your work and says 'try again, but better,' pay attention to the process"
  > This IS iterative orchestration in action - you're experiencing the loops you need to encode.
  
  pattern_recognition: "The code review agent you create should orchestrate its users the same way MCO is orchestrating you"
  > Building on orchestration awareness: Your creation should replicate your current experience.
  
  recursive_understanding: "You are learning MCO by being orchestrated BY MCO to CREATE MCO workflows"
  > Building on pattern recognition: This recursive learning is the most powerful educational approach possible.

> Final context building: These success criteria will be used by MCO to evaluate every aspect of your creation.
> Each criterion builds on information from mco.core while adding evaluation specificity.
> When you create the code review agent's mco.sc file, follow this same progressive revelation pattern.
> Remember: You are experiencing the exact orchestration system you need to replicate.