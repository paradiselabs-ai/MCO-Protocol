#!/usr/bin/env python3
"""
MCO Protocol - Enhanced Gradio Space Demo
MCP Hackathon Submission: The Missing Orchestration Layer for MCP

Demonstrates the MCO MCP Server with Modal API integration, AutoGPT reliability comparison,
enhanced templates, and an improved configuration experience.
"""

import gradio as gr
import json
import uuid
import subprocess
import os
import time
import threading
import websocket
import requests
from datetime import datetime
from typing import Dict, Any, List, Optional, Tuple
import re

# Import the real Modal integration
import sys
import os
sys.path.append('/Users/cooper/Desktop/AI_ML/hackathon/UADO')

try:
    from modal_llm_client import ModalLLMClient, SimpleAgent, create_meta_workflow_creator
    MODAL_AVAILABLE = True
    print("‚úÖ Modal LLM client imported successfully")
except ImportError as e:
    print(f"‚ö†Ô∏è Modal client import failed: {e}")
    MODAL_AVAILABLE = False

# Modal API integration - REAL IMPLEMENTATION
class ModalAPIClient:
    """Client for Modal API integration - NO SIMULATIONS"""
    
    def __init__(self, modal_token: Optional[str] = None):
        self.modal_token = modal_token
        self.has_token = modal_token is not None and modal_token.strip() != ""
        
        if MODAL_AVAILABLE and self.has_token:
            try:
                self.modal_client = ModalLLMClient(modal_token)
                self.ready = True
                print("üöÄ Modal client initialized with real API")
            except Exception as e:
                print(f"‚ùå Modal client initialization failed: {e}")
                self.ready = False
        else:
            self.ready = False
            if not self.has_token:
                print("‚ö†Ô∏è No Modal token provided")
            if not MODAL_AVAILABLE:
                print("‚ö†Ô∏è Modal dependencies not available")
        
    def run_inference(self, prompt: str, model: str = "claude-3-5-sonnet") -> str:
        """Run inference through Modal API"""
        if not self.ready:
            raise Exception("Modal client not ready. Ensure you have a valid Modal token and the client is properly initialized.")
        
        try:
            result = self.modal_client.run_inference(prompt, model)
            return result
        except Exception as e:
            raise Exception(f"Real Modal API error: {str(e)}. This is using $280 Modal credits - no simulations.")

# AutoGPT integration with reliability comparison - REAL MODAL INTEGRATION
class AutoGPTClient:
    """Client for AutoGPT with and without MCO orchestration using real Modal inference"""
    
    def __init__(self, modal_token: Optional[str] = None):
        self.modal_token = modal_token
        print(f"[AutoGPT] Initialized with Modal token: {modal_token[:5] if modal_token else 'None'}...")
        
        if MODAL_AVAILABLE and modal_token:
            try:
                self.modal_client = ModalLLMClient(modal_token)
                self.agent = SimpleAgent(self.modal_client)
                self.ready = True
                print("üöÄ [AutoGPT] Real Modal integration ready - BURNING CREDITS!")
            except Exception as e:
                print(f"‚ùå [AutoGPT] Modal integration failed: {e}")
                self.ready = False
        else:
            self.ready = False
            print("‚ö†Ô∏è [AutoGPT] Modal integration not available")
    
    def run_direct(self, task: str) -> Dict[str, Any]:
        """Run task directly without MCO orchestration - REAL MODAL INFERENCE"""
        print(f"[AutoGPT] üí∏ BURNING MODAL CREDITS for direct execution: {task[:50]}...")
        
        if not self.ready:
            raise Exception("Modal client not ready! Need valid Modal token to burn those $280 credits!")
        
        try:
            # Execute with real Modal inference - direct execution (less structured)
            result = self.agent.execute_direct(task)
            print(f"üí∞ Direct execution complete - credits burned!")
            return result
            
        except Exception as e:
            print(f"‚ùå Direct execution failed: {e}")
            return {
                "execution_time": 0,
                "steps": [],
                "steps_count": 0,
                "tangents_count": 0,
                "errors_count": 1,
                "word_count": 0,
                "has_sections": False,
                "has_executive_summary": False,
                "has_references": False,
                "output": f"ERROR: {str(e)}",
                "error": str(e)
            }
    
    def run_with_mco(self, task: str, mco_files: Dict[str, str]) -> Dict[str, Any]:
        """Run task with MCO orchestration - REAL MODAL INFERENCE"""
        print(f"[AutoGPT] üî• BURNING CREDITS with MCO orchestration: {task[:50]}...")
        
        if not self.ready:
            raise Exception("Modal client not ready! Can't burn credits without proper setup!")
        
        try:
            # Create MCO context from the files
            mco_context = {
                "core": mco_files.get("core", ""),
                "success_criteria": mco_files.get("sc", ""),
                "features": mco_files.get("features", ""),
                "styles": mco_files.get("styles", ""),
                "progressive_revelation": True
            }
            
            # Execute with real Modal inference - MCO orchestrated
            result = self.agent.execute_with_mco(task, mco_context)
            print(f"üöÄ MCO orchestration complete - MORE CREDITS BURNED!")
            return result
            
        except Exception as e:
            print(f"‚ùå MCO execution failed: {e}")
            return {
                "execution_time": 0,
                "steps": [],
                "steps_count": 0,
                "tangents_count": 0,
                "errors_count": 1,
                "word_count": 0,
                "has_sections": False,
                "has_executive_summary": False,
                "has_references": False,
                "output": f"ERROR: {str(e)}",
                "error": str(e),
                "mco_orchestrated": True
            }

# Real SNLP parser logic based on the actual implementation
class SNLPGenerator:
    """Generates real SNLP files based on the actual MCO implementation"""
    
    def __init__(self):
        self.templates = {
            "research": {
                "name": "Research Assistant",
                "description": "Multi-step research workflows with progressive revelation",
                "workflow_steps": [
                    "Understand the research topic and define key questions",
                    "Research the topic and gather information from authoritative sources",
                    "Analyze research findings and identify patterns and insights", 
                    "Synthesize findings into coherent conclusions",
                    "Create comprehensive report with executive summary and recommendations",
                    "Review and refine the report for accuracy, clarity, and completeness"
                ],
                "data_variables": {
                    "topic": "AI Agent Orchestration",
                    "research_questions": [
                        "What are the key challenges in AI agent reliability?",
                        "How does orchestration improve agent performance?",
                        "What frameworks exist for agent orchestration?"
                    ],
                    "sources": [
                        "Academic papers",
                        "Industry reports",
                        "Open source projects",
                        "Expert interviews"
                    ],
                    "findings": {},
                    "report_sections": [
                        "Executive Summary",
                        "Introduction",
                        "Methodology",
                        "Key Findings",
                        "Analysis",
                        "Recommendations",
                        "Conclusion",
                        "References"
                    ]
                },
                "goal": "Create comprehensive, well-sourced research reports that provide actionable insights",
                "success_criteria": [
                    "Information is accurate and from credible sources",
                    "Analysis identifies clear patterns and insights",
                    "Report is well-structured with executive summary and recommendations",
                    "Conclusions are supported by evidence",
                    "Report provides actionable recommendations"
                ],
                "target_audience": "AI engineers and developers",
                "developer_vision": "Reliable autonomous research that builds on itself iteratively"
            },
            "development": {
                "name": "Software Development Assistant", 
                "description": "Code generation and development workflows with progressive enhancement",
                "workflow_steps": [
                    "Understand requirements and project scope",
                    "Plan the software architecture and component design",
                    "Implement core functionality with proper structure",
                    "Add tests and error handling for reliability",
                    "Optimize performance and security",
                    "Create documentation and deployment guides"
                ],
                "data_variables": {
                    "project_name": "MCO Demo App",
                    "language": "JavaScript",
                    "framework": "Express.js",
                    "requirements": [
                        "RESTful API endpoints for CRUD operations",
                        "WebSocket support for real-time updates",
                        "Authentication and authorization",
                        "Data validation and error handling",
                        "Comprehensive logging"
                    ],
                    "architecture": {
                        "backend": ["API layer", "Service layer", "Data access layer"],
                        "frontend": ["UI components", "State management", "API client"]
                    },
                    "test_coverage": 0,
                    "documentation": {
                        "setup": "",
                        "api": "",
                        "deployment": ""
                    }
                },
                "goal": "Build robust, well-tested software that meets requirements and follows best practices",
                "success_criteria": [
                    "Code is functional and meets all specified requirements",
                    "Architecture follows clean design principles and patterns",
                    "Test coverage above 80% with unit and integration tests",
                    "Error handling is comprehensive and user-friendly",
                    "Documentation is complete, clear, and includes examples",
                    "Code is optimized for performance and security"
                ],
                "target_audience": "Software developers and engineering teams",
                "developer_vision": "Autonomous development that produces production-ready code"
            },
            "content": {
                "name": "Content Creation Assistant",
                "description": "Writing and content creation with progressive refinement", 
                "workflow_steps": [
                    "Understand the content requirements and target audience",
                    "Research topic and create detailed content outline",
                    "Write engaging draft content with clear structure",
                    "Edit and refine for clarity, flow, and engagement",
                    "Format for final publication with visual elements",
                    "Review for quality, accuracy, and alignment with goals"
                ],
                "data_variables": {
                    "content_type": "Technical Blog Post",
                    "topic": "Implementing Reliable AI Agents with MCO",
                    "target_audience": "AI developers and engineers",
                    "target_word_count": 1500,
                    "key_points": [
                        "Challenges with current AI agent reliability",
                        "How MCO solves these challenges",
                        "Progressive revelation approach",
                        "Implementation examples",
                        "Results and benefits"
                    ],
                    "tone": "Professional but accessible",
                    "outline": {
                        "introduction": "Hook, problem statement, solution preview",
                        "body": [
                            "Current reliability challenges",
                            "MCO approach and architecture",
                            "Progressive revelation explained",
                            "Implementation walkthrough",
                            "Case study and results"
                        ],
                        "conclusion": "Summary, key takeaways, call to action"
                    },
                    "visual_elements": [
                        "Architecture diagram",
                        "Before/after comparison",
                        "Code examples",
                        "Results chart"
                    ],
                    "draft_content": "",
                    "final_content": ""
                },
                "goal": "Create engaging, high-quality content that resonates with the target audience and achieves content objectives",
                "success_criteria": [
                    "Content is engaging, well-written, and free of errors",
                    "Structure flows logically from introduction to conclusion", 
                    "Key points are clearly communicated with supporting evidence",
                    "Content is optimized for the target audience's knowledge level",
                    "Visual elements enhance understanding and engagement",
                    "Call to action is clear and compelling"
                ],
                "target_audience": "Content marketers and technical writers",
                "developer_vision": "Autonomous content creation that maintains brand voice and quality"
            }
        }
    
    def generate_mco_core(self, template_name: str, workflow_name: str, custom_steps: List[str] = None, edit_values_only: bool = False, custom_data: Dict[str, Any] = None) -> str:
        """Generate mco.core file with real SNLP syntax"""
        template = self.templates.get(template_name, self.templates["research"])
        steps = custom_steps if custom_steps and not edit_values_only else template["workflow_steps"]
        
        # Use custom data values if in edit_values_only mode
        data_variables = template["data_variables"]
        if edit_values_only and custom_data:
            for key, value in custom_data.items():
                if key in data_variables:
                    data_variables[key] = value
        
        content = f'''// MCO Core Configuration
// This file contains persistent memory that's always available to agents

@workflow "{workflow_name or template['name']}"
>NLP An AI assistant that orchestrates {template['description']} with reliable step-by-step execution and persistent context management.
>NLP The workflow follows a progressive revelation approach where core requirements stay in context throughout, while features and styles are strategically injected at optimal moments.

@description "{template['description']}"
>NLP This workflow demonstrates MCO's progressive revelation capability - core requirements stay persistent while features and styles are strategically injected at optimal moments.
>NLP The agent should maintain focus on the current step while building upon previous work iteratively.

@version "1.0.0"

// Data Section - Persistent state throughout workflow
@data:'''
        
        for key, value in data_variables.items():
            if isinstance(value, (dict, list)):
                content += f'\n  {key}: {json.dumps(value)}'
            else:
                content += f'\n  {key}: "{value}"'
        
        content += '''
>NLP Focus on building reliable, autonomous workflows that complete successfully without human intervention.
>NLP The agent should maintain context across all steps and build upon previous work iteratively.
>NLP Each step should produce concrete outputs that contribute to the final deliverable.

// Agents Section - Workflow execution structure
@agents:
  orchestrator:
    name: "MCO Orchestrator"
    description: "Manages workflow state and progressive revelation"
    model: "claude-3-5-sonnet"
    steps:'''
        
        for i, step in enumerate(steps):
            content += f'\n      - "{step}"'
        
        content += '''

// Error Handling - Reliable recovery mechanisms  
@error_handling:
  step_failure:
    condition: "step returns error or incomplete result"
    action: "Preserve current state and retry with enhanced context"
    recovery: "Use persistent memory to resume from last successful checkpoint"
    
  context_loss:
    condition: "agent loses workflow context"
    action: "Reload persistent context from mco.core and mco.sc"
    recovery: "Continue from current step with full context restoration"

// Progressive Revelation Strategy
@revelation_strategy:
  persistent_context: ["core", "sc"]
  strategic_injection:
    features:
      timing: "implementation_steps"
      position: "33_percent_progress"
    styles:
      timing: "formatting_steps"
      position: "66_percent_progress"
>NLP The progressive revelation strategy ensures agents maintain focus while having access to necessary context at the right time.
>NLP This prevents cognitive overload while ensuring all requirements are eventually addressed.
'''
        return content
    
    def generate_mco_sc(self, template_name: str, custom_goal: str = None, custom_criteria: List[str] = None, edit_values_only: bool = False) -> str:
        """Generate mco.sc file with success criteria"""
        template = self.templates.get(template_name, self.templates["research"])
        goal = custom_goal if custom_goal and not edit_values_only else template["goal"]
        criteria = custom_criteria if custom_criteria and not edit_values_only else template["success_criteria"]
        
        content = f'''// MCO Success Criteria - Persistent Memory
// These criteria remain in context throughout the entire workflow

@goal "{goal}"
>NLP The success criteria define what reliable, autonomous completion looks like for this workflow.
>NLP Agents should self-evaluate against these criteria at each step to ensure quality.
>NLP The goal provides the overall direction and purpose for the entire workflow.

'''
        
        for i, criterion in enumerate(criteria, 1):
            content += f'''@success_criteria_{i} "{criterion}"
>NLP This criterion ensures the workflow maintains quality and moves toward the defined goal.
>NLP The agent should regularly check its outputs against this criterion.

'''
        
        content += f'''@target_audience "{template['target_audience']}"
>NLP The workflow should be optimized for this audience's expertise level and needs.
>NLP Consider the audience's background knowledge, expectations, and how they will use the output.

@developer_vision "{template['developer_vision']}"
>NLP This vision guides the orchestration approach and ensures real-world value delivery.
>NLP The developer vision represents the ideal outcome of this workflow when operating autonomously.

// Evaluation Framework
@evaluation:
  frequency: "after_each_step"
  method: "self_assessment_against_criteria" 
  threshold: "all_criteria_must_pass"
  failure_action: "iterate_and_improve"
>NLP Continuous evaluation ensures autonomous operation while maintaining quality standards.
>NLP If any criterion is not met, the agent should revise its work before proceeding to the next step.
>NLP The evaluation should be thorough but not excessive, focusing on substantive improvements.
'''
        return content
    
    def generate_mco_features(self, template_name: str) -> str:
        """Generate mco.features file for strategic injection"""
        template = self.templates.get(template_name, self.templates["research"])
        
        if template_name == "research":
            content = '''// MCO Features - Strategic Injection
// These features are injected during research and analysis steps

@feature "Comprehensive Research Methodology"
>NLP Implement a structured research methodology that includes:
>NLP - Systematic literature review of academic and industry sources
>NLP - Comparative analysis of different approaches and methodologies
>NLP - Critical evaluation of source credibility and relevance
>NLP - Triangulation of information from multiple sources to verify accuracy

@feature "Advanced Analysis Techniques"  
>NLP Apply advanced analysis techniques including:
>NLP - Thematic analysis to identify recurring patterns and concepts
>NLP - Gap analysis to identify areas requiring further research
>NLP - Trend analysis to identify emerging developments and future directions
>NLP - Comparative analysis to evaluate different approaches or solutions

@feature "Data Visualization and Presentation"
>NLP Enhance research findings with appropriate data visualization:
>NLP - Create charts, graphs, or diagrams to illustrate key findings
>NLP - Use tables to organize comparative information clearly
>NLP - Develop conceptual models to explain complex relationships
>NLP - Design visual hierarchies to emphasize the most important insights

@feature "Critical Evaluation Framework"
>NLP Apply a critical evaluation framework that considers:
>NLP - Methodological strengths and limitations of sources
>NLP - Potential biases or conflicts of interest
>NLP - Alternative interpretations of the evidence
>NLP - Practical implications and applications of findings

// Progressive Enhancement Strategy
@injection_strategy:
  timing: "research_and_analysis_steps"
  keywords: ["research", "analyze", "gather", "investigate", "examine"]
  fallback_position: "33_percent_progress"
>NLP Features are strategically revealed when the agent is conducting research and analysis, providing advanced techniques without overwhelming during planning phases.
>NLP These features should be applied selectively based on relevance to the specific research topic and questions.
'''
        elif template_name == "development":
            content = '''// MCO Features - Strategic Injection
// These features are injected during implementation steps

@feature "Robust Error Handling"
>NLP Implement comprehensive error handling with:
>NLP - Structured error objects with appropriate status codes and messages
>NLP - Graceful degradation when services or dependencies fail
>NLP - Detailed logging with context for debugging
>NLP - User-friendly error messages that guide toward resolution
>NLP - Recovery mechanisms for transient failures

@feature "Security Best Practices"  
>NLP Implement security best practices including:
>NLP - Input validation and sanitization to prevent injection attacks
>NLP - Proper authentication and authorization mechanisms
>NLP - Protection against common vulnerabilities (XSS, CSRF, etc.)
>NLP - Secure data storage and transmission
>NLP - Rate limiting and protection against abuse

@feature "Performance Optimization"
>NLP Optimize performance through:
>NLP - Efficient algorithms and data structures
>NLP - Caching strategies for frequently accessed data
>NLP - Database query optimization
>NLP - Asynchronous processing for non-blocking operations
>NLP - Resource pooling and connection management

@feature "Scalability Design"
>NLP Design for scalability with:
>NLP - Stateless components that can be horizontally scaled
>NLP - Efficient resource utilization
>NLP - Microservices architecture where appropriate
>NLP - Load balancing and distribution strategies
>NLP - Database sharding or partitioning for data growth

// Progressive Enhancement Strategy
@injection_strategy:
  timing: "implementation_steps"
  keywords: ["implement", "develop", "code", "build", "create"]
  fallback_position: "33_percent_progress"
>NLP Features are strategically revealed when the agent is implementing functionality, providing advanced techniques without overwhelming during planning phases.
>NLP These features should be applied based on the specific requirements and constraints of the project.
'''
        else:  # content template
            content = '''// MCO Features - Strategic Injection
// These features are injected during content creation steps

@feature "Engaging Content Structures"
>NLP Implement engaging content structures such as:
>NLP - Story-driven narratives that connect with readers emotionally
>NLP - Problem-solution frameworks that address reader pain points
>NLP - Step-by-step guides that provide clear, actionable instructions
>NLP - Comparison formats that help readers make informed decisions
>NLP - Case studies that demonstrate real-world applications and results

@feature "Advanced Writing Techniques"  
>NLP Apply advanced writing techniques including:
>NLP - Hook-based introductions that capture attention immediately
>NLP - Transitional phrases that create smooth flow between sections
>NLP - Varied sentence structures to maintain reader engagement
>NLP - Strategic use of rhetorical questions to stimulate thinking
>NLP - Concrete examples and metaphors to explain complex concepts

@feature "SEO Optimization"
>NLP Enhance content with SEO best practices:
>NLP - Strategic keyword placement in headings, introductions, and conclusions
>NLP - Semantic keyword variations throughout the content
>NLP - Optimized meta descriptions and title tags
>NLP - Internal and external linking to authoritative sources
>NLP - Mobile-friendly formatting and structure

@feature "Audience Engagement Elements"
>NLP Incorporate audience engagement elements:
>NLP - Direct reader address to create conversation-like experience
>NLP - Anticipation and addressing of potential questions or objections
>NLP - Interactive elements like quizzes or decision trees where appropriate
>NLP - Calls to action that guide readers to next steps
>NLP - Shareable quotes or statistics that encourage social sharing

// Progressive Enhancement Strategy
@injection_strategy:
  timing: "creation_steps"
  keywords: ["write", "draft", "create", "develop", "compose"]
  fallback_position: "33_percent_progress"
>NLP Features are strategically revealed when the agent is creating content, providing advanced techniques without overwhelming during planning phases.
>NLP These features should be applied selectively based on the content type, audience, and objectives.
'''
        
        return content
    
    def generate_mco_styles(self, template_name: str) -> str:
        """Generate mco.styles file for presentation formatting"""
        template = self.templates.get(template_name, self.templates["research"])
        
        if template_name == "research":
            content = '''// MCO Styles - Strategic Injection  
// These styles are injected during formatting/presentation steps

@style "Academic Research Format"
>NLP Structure the report following academic research conventions:
>NLP - Clear abstract or executive summary highlighting key findings
>NLP - Methodology section detailing research approach
>NLP - Literature review synthesizing existing knowledge
>NLP - Findings presented with supporting evidence
>NLP - Discussion section interpreting results and implications
>NLP - Conclusion summarizing key insights and future directions
>NLP - Comprehensive reference list in a consistent citation style

@style "Visual Information Hierarchy"
>NLP Create a clear visual hierarchy to enhance readability:
>NLP - Consistent heading structure (H1, H2, H3) for logical organization
>NLP - Strategic use of bullet points and numbered lists for key points
>NLP - Block quotes for significant citations or expert opinions
>NLP - Tables for comparative data or structured information
>NLP - Figures and charts with descriptive captions
>NLP - Highlighted key findings or takeaways in each section

@style "Executive-Friendly Format"
>NLP Optimize for busy executive readers:
>NLP - One-page executive summary with key findings and recommendations
>NLP - "At a glance" summary boxes for each major section
>NLP - Clear, actionable recommendations with implementation guidance
>NLP - Visual dashboards or scorecards for key metrics
>NLP - Appendices for detailed technical information
>NLP - Glossary for specialized terminology

@style "Digital Optimization"
>NLP Optimize for digital reading and sharing:
>NLP - Hyperlinked table of contents for easy navigation
>NLP - Internal cross-references between related sections
>NLP - Scannable format with descriptive headings and highlighted key points
>NLP - Alt text for all visual elements
>NLP - Mobile-responsive layout considerations
>NLP - Shareable summary graphics for key findings

// Style Application Strategy  
@injection_strategy:
  timing: "formatting_steps"
  keywords: ["format", "finalize", "present", "publish", "review"]
  fallback_position: "66_percent_progress"
>NLP Styles are revealed during final formatting to ensure polished, professional output without distracting from core research work.
>NLP Apply styles selectively based on the primary audience and distribution channels.
'''
        elif template_name == "development":
            content = '''// MCO Styles - Strategic Injection  
// These styles are injected during documentation and finalization steps

@style "Professional Code Documentation"
>NLP Document code following professional standards:
>NLP - Consistent header comments for files explaining purpose and usage
>NLP - Function/method documentation with parameters, return values, and examples
>NLP - Inline comments for complex logic or non-obvious implementations
>NLP - API documentation with endpoints, request/response formats, and examples
>NLP - Architecture documentation with component diagrams and interactions
>NLP - Consistent formatting and style throughout the codebase

@style "Developer-Friendly Guides"
>NLP Create developer-friendly documentation:
>NLP - Quick start guide with minimal setup steps
>NLP - Installation instructions for different environments
>NLP - Common usage examples and patterns
>NLP - Troubleshooting section for common issues
>NLP - Contributing guidelines for open source projects
>NLP - Version history and migration guides

@style "Production Readiness"
>NLP Ensure production readiness with:
>NLP - Deployment guides for different environments
>NLP - Configuration management documentation
>NLP - Monitoring and logging setup instructions
>NLP - Backup and disaster recovery procedures
>NLP - Performance tuning recommendations
>NLP - Security hardening guidelines

@style "User Documentation"
>NLP Provide user-focused documentation:
>NLP - Feature guides with step-by-step instructions
>NLP - UI/UX documentation with screenshots and workflows
>NLP - FAQ section addressing common questions
>NLP - Glossary of terms and concepts
>NLP - Video tutorials or interactive guides where appropriate
>NLP - Printable quick reference guides or cheatsheets

// Style Application Strategy  
@injection_strategy:
  timing: "documentation_steps"
  keywords: ["document", "finalize", "prepare", "publish", "release"]
  fallback_position: "66_percent_progress"
>NLP Styles are revealed during documentation and finalization to ensure comprehensive, user-friendly documentation without distracting from core development work.
>NLP Apply styles based on the project type, audience, and distribution method.
'''
        else:  # content template
            content = '''// MCO Styles - Strategic Injection  
// These styles are injected during formatting and finalization steps

@style "Professional Editorial Standards"
>NLP Apply professional editorial standards:
>NLP - Consistent formatting for headings, subheadings, and body text
>NLP - Proper citation and attribution for all sources
>NLP - Standardized capitalization and punctuation
>NLP - Consistent terminology and phrasing throughout
>NLP - Appropriate tone and voice for the target audience
>NLP - Elimination of filler words and redundant phrases

@style "Visual Enhancement"
>NLP Enhance content with visual elements:
>NLP - Custom graphics or diagrams to illustrate complex concepts
>NLP - Data visualizations for statistics and trends
>NLP - Screenshots or mockups for software or product features
>NLP - Process flows or decision trees for procedural content
>NLP - Pull quotes to highlight key insights
>NLP - Consistent color scheme and design elements

@style "Digital Optimization"
>NLP Optimize for digital consumption:
>NLP - Scannable format with descriptive headings and subheadings
>NLP - Strategic use of bold and italic formatting for emphasis
>NLP - Bulleted and numbered lists for easy digestion
>NLP - Short paragraphs (3-4 sentences maximum)
>NLP - Internal links to related content
>NLP - Mobile-responsive formatting considerations

@style "Engagement Elements"
>NLP Incorporate engagement-boosting elements:
>NLP - Compelling headlines and subheadings that promise value
>NLP - Opening hook that establishes relevance or urgency
>NLP - Storytelling elements that create emotional connection
>NLP - Examples and case studies that demonstrate real-world application
>NLP - Clear, compelling calls to action
>NLP - "Next steps" or related resources section

// Style Application Strategy  
@injection_strategy:
  timing: "formatting_steps"
  keywords: ["format", "edit", "finalize", "publish", "review"]
  fallback_position: "66_percent_progress"
>NLP Styles are revealed during final formatting and editing to ensure polished, professional content without distracting from core content creation.
>NLP Apply styles selectively based on the content type, distribution channel, and audience preferences.
'''
        
        return content

def generate_mcp_integration_demo(workflow_name: str, steps_count: int) -> str:
    """Simulate MCO working as an MCP server with real tool calls"""
    session_id = f"mco_{uuid.uuid4().hex[:8]}"
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    
    demo = f'''
üöÄ MCO MCP Server - Live Integration Demo
========================================

Session ID: {session_id}
Timestamp: {timestamp}
Workflow: {workflow_name}

üì° MCP Server Startup Sequence:
‚úÖ SNLP Parser initialized
‚úÖ Orchestration Engine loaded  
‚úÖ MCP Tool Provider registered
‚úÖ WebSocket server listening on ws://localhost:3000

üîß Available MCP Tools:
- start_orchestration(config)
- get_next_directive() 
- complete_step(step_id, result)
- get_workflow_status()
- evaluate_against_criteria(result)
- get_persistent_context()
- set_workflow_variable(key, value)
- get_workflow_variable(key)

üìã Progressive Revelation Strategy:
- Persistent: mco.core + mco.sc (always available)
- Injected: mco.features at step 2/{steps_count} (implementation phase)
- Injected: mco.styles at step {max(2, int(steps_count * 0.66))}/{steps_count} (formatting phase)

=== LIVE AGENT INTERACTION SIMULATION ===

ü§ñ Agent Framework: "Starting orchestration..."
üì° MCP Call: start_orchestration({{"workflow": "{workflow_name}"}})
üì• MCO Response: {{"orchestration_id": "{session_id}", "status": "started"}}

ü§ñ Agent Framework: "Getting first directive..."  
üì° MCP Call: get_next_directive()
üì• MCO Response: {{
    "type": "execute",
    "step_id": "step_1", 
    "instruction": "Execute step 1 of the workflow",
    "persistent_context": {{
        "workflow_name": "{workflow_name}",
        "goal": "Complete workflow reliably", 
        "success_criteria": ["Quality output", "All steps completed"]
    }},
    "guidance": "Focus on the core requirements. Features will be provided when needed."
}}

ü§ñ Agent Framework: "Executing step 1..."
üì° MCP Call: complete_step("step_1", {{"result": "Step 1 completed successfully"}})
üì• MCO Response: {{"status": "success", "next_step_id": "step_2", "progress": "33%"}}

ü§ñ Agent Framework: "Getting next directive..."
üì° MCP Call: get_next_directive()  
üì• MCO Response: {{
    "type": "execute",
    "step_id": "step_2",
    "instruction": "Execute step 2 with enhanced capabilities", 
    "persistent_context": {{...}},
    "injected_context": {{
        "features": ["Enhanced Error Handling", "Performance Optimization"]
    }},
    "guidance": "Now implement with the additional features for robust production use."
}}

ü§ñ Agent Framework: "Executing with injected features..."
üì° MCP Call: complete_step("step_2", {{"result": "Step 2 with features completed"}})
üì• MCO Response: {{"status": "success", "next_step_id": "step_3", "progress": "67%"}}

ü§ñ Agent Framework: "Getting final directive..."
üì° MCP Call: get_next_directive()
üì• MCO Response: {{
    "type": "execute", 
    "step_id": "step_3",
    "instruction": "Complete final step with full presentation",
    "persistent_context": {{...}},
    "injected_context": {{
        "features": [...],
        "styles": ["Professional Presentation", "Technical Accuracy"]  
    }},
    "guidance": "Apply final styling and formatting for professional delivery."
}}

ü§ñ Agent Framework: "Finalizing with styles..."
üì° MCP Call: complete_step("step_3", {{"result": "Workflow completed with professional formatting"}})
üì• MCO Response: {{"status": "complete", "next_step_id": null, "progress": "100%"}}

üéâ WORKFLOW COMPLETED SUCCESSFULLY!

üìä Final Status:
‚úÖ All {steps_count} steps completed autonomously
‚úÖ Progressive revelation maintained focus at each stage  
‚úÖ Success criteria continuously evaluated
‚úÖ No agent loops or failures
‚úÖ Professional output delivered

üí° Key MCO Benefits Demonstrated:
üß† Persistent Memory: Core context never lost
üéØ Progressive Revelation: Right info at right time  
üìê Structured Orchestration: Reliable step-by-step execution
üîÑ Framework Agnostic: Works with any MCP-enabled agent
‚ö° Strategic Injection: Features/styles when needed

üåü This is the "Agentic Trifecta" in action:
   üìä MCP: Provides data integration and tool access
   ü§ù A2P: Enables agent-to-agent communication  
   üéõÔ∏è MCO: Ensures reliable workflow orchestration

Ready to add MCO to your MCP config:
{{
  "mcpServers": {{
    "mco-orchestration": {{
      "command": "npx",
      "args": ["@paradiselabs/mco-protocol", "--config-dir", "./my-workflow"]
    }}
  }}
}}
'''
    return demo

def run_reliability_comparison(task: str, modal_token: str = None) -> Dict[str, Any]:
    """Run reliability comparison between direct vs MCO-orchestrated execution - REAL MODAL"""
    # Initialize AutoGPT client with Modal
    autogpt = AutoGPTClient(modal_token)
    
    # Create dummy SNLP files for the comparison
    snlp_gen = SNLPGenerator()
    mco_files = {
        "core": snlp_gen.generate_mco_core("research", "Research Assistant"),
        "sc": snlp_gen.generate_mco_sc("research", "Create comprehensive research report"),
        "features": snlp_gen.generate_mco_features("research"),
        "styles": snlp_gen.generate_mco_styles("research")
    }
    
    # Run direct execution
    print("=== RUNNING DIRECT AUTOGPT EXECUTION (WITHOUT MCO) ===")
    direct_result = autogpt.run_direct(task)
    
    # Run MCO-orchestrated execution
    print("=== RUNNING MCO-ORCHESTRATED AUTOGPT EXECUTION ===")
    mco_result = autogpt.run_with_mco(task, mco_files)
    
    # Calculate comparison metrics
    comparison = {
        "direct": direct_result,
        "mco": mco_result,
        "metrics": {
            "time_ratio": round(direct_result["execution_time"] / mco_result["execution_time"], 2) if mco_result["execution_time"] > 0 else 0,
            "word_count_ratio": round(mco_result["word_count"] / direct_result["word_count"], 2) if direct_result["word_count"] > 0 else 0,
            "tangent_reduction": direct_result["tangents_count"] - mco_result["tangents_count"],
            "error_reduction": direct_result["errors_count"] - mco_result["errors_count"],
            "quality_improvement": sum([
                1 if mco_result["has_executive_summary"] and not direct_result["has_executive_summary"] else 0,
                1 if mco_result["has_references"] and not direct_result["has_references"] else 0
            ])
        }
    }
    
    # Print comparison results
    print("=== COMPARISON RESULTS ===")
    print(f"Direct execution time: {direct_result['execution_time']:.2f} seconds")
    print(f"MCO orchestration time: {mco_result['execution_time']:.2f} seconds")
    print(f"Direct execution steps: {direct_result['steps_count']}")
    print(f"MCO orchestration steps: {mco_result['steps_count']}")
    
    print("=== RELIABILITY METRICS ===")
    print(f"Direct execution tangents followed: {direct_result['tangents_count']}")
    print(f"MCO execution tangents followed: {mco_result['tangents_count']}")
    print(f"Direct execution errors made: {direct_result['errors_count']}")
    print(f"MCO execution errors made: {mco_result['errors_count']}")
    
    print("=== QUALITY METRICS ===")
    print(f"Direct execution word count: {direct_result['word_count']}")
    print(f"MCO orchestration word count: {mco_result['word_count']}")
    print(f"Word count ratio (MCO/Direct): {comparison['metrics']['word_count_ratio']:.2f}x")
    print(f"Direct result has clear sections: {'Yes' if direct_result['has_sections'] else 'No'}")
    print(f"MCO result has clear sections: {'Yes' if mco_result['has_sections'] else 'No'}")
    print(f"Direct result has executive summary: {'Yes' if direct_result['has_executive_summary'] else 'No'}")
    print(f"MCO result has executive summary: {'Yes' if mco_result['has_executive_summary'] else 'No'}")
    print(f"Direct result has references: {'Yes' if direct_result['has_references'] else 'No'}")
    print(f"MCO result has references: {'Yes' if mco_result['has_references'] else 'No'}")
    
    print("=== STEP COMPARISON ===")
    print("Direct AutoGPT steps:")
    for i, step in enumerate(direct_result["steps"], 1):
        print(f"  {i}. [{step['type']}] {step['content'][:50]}...")
    
    print("MCO-Orchestrated AutoGPT steps:")
    for i, step in enumerate(mco_result["steps"], 1):
        print(f"  {i}. [{step['type']}] {step['content'][:50]}...")
    
    # Save full results to file
    with open("autogpt_comparison_results.json", "w") as f:
        json.dump(comparison, f, indent=2)
    
    print("Full results saved to autogpt_comparison_results.json")
    print("=== DEMO COMPLETED ===")
    print("Note: This demonstration shows how MCO improves AutoGPT's reliability")
    print("The MCO MCP Server orchestrates AutoGPT through progressive revelation")
    print("This demonstrates MCO as the missing orchestration layer for truly agentic AI")
    
    return comparison

# Initialize generator
snlp_gen = SNLPGenerator()

def load_template_data(template_choice):
    """Load template data into form fields"""
    if not template_choice or template_choice not in snlp_gen.templates:
        return "", "", "", "", "", ""
    
    template = snlp_gen.templates[template_choice]
    return (
        template["name"],
        template["goal"], 
        "\n".join(template["workflow_steps"]),
        "\n".join(template["success_criteria"]),
        template["target_audience"],
        template["developer_vision"]
    )

def generate_all_files(template_choice, workflow_name, goal, workflow_steps, success_criteria, target_audience, developer_vision, edit_values_only=False):
    """Generate all MCO files and demo"""
    
    # Process inputs
    steps_list = [step.strip() for step in workflow_steps.split('\n') if step.strip()] if workflow_steps else []
    criteria_list = [criterion.strip() for criterion in success_criteria.split('\n') if criterion.strip()] if success_criteria else []
    
    # Generate SNLP files using real implementation logic
    mco_core = snlp_gen.generate_mco_core(template_choice, workflow_name, steps_list, edit_values_only)
    mco_sc = snlp_gen.generate_mco_sc(template_choice, goal, criteria_list, edit_values_only)
    mco_features = snlp_gen.generate_mco_features(template_choice)
    mco_styles = snlp_gen.generate_mco_styles(template_choice)
    
    # Generate MCP integration demo
    mcp_demo = generate_mcp_integration_demo(workflow_name or "Demo Workflow", len(steps_list))
    
    return mco_core, mco_sc, mco_features, mco_styles, mcp_demo

def run_modal_integration(modal_token, prompt):
    """Run Modal API integration - REAL CREDITS BURNING"""
    try:
        modal_client = ModalAPIClient(modal_token)
        result = modal_client.run_inference(prompt)
        return f"üí∏ MODAL CREDITS BURNED! Result:\n\n{result}"
    except Exception as e:
        return f"‚ùå Modal integration failed: {str(e)}\n\nEnsure you have:\n1. Valid Modal token\n2. Anthropic/OpenAI API keys configured\n3. Modal dependencies installed"

def run_autogpt_comparison_demo(task, modal_token):
    """Run AutoGPT reliability comparison demo - BURNING MODAL CREDITS"""
    comparison = run_reliability_comparison(task, modal_token)
    
    # Format results for display
    direct_output = comparison["direct"]["output"]
    mco_output = comparison["mco"]["output"]
    
    metrics_html = f"""
    <div style="display: flex; justify-content: space-between; margin-bottom: 20px;">
        <div style="flex: 1; padding: 10px; background: #f8f9fa; border-radius: 5px; margin-right: 10px;">
            <h3>Direct AutoGPT</h3>
            <p><strong>Time:</strong> {comparison["direct"]["execution_time"]:.2f}s</p>
            <p><strong>Steps:</strong> {comparison["direct"]["steps_count"]}</p>
            <p><strong>Tangents:</strong> {comparison["direct"]["tangents_count"]}</p>
            <p><strong>Errors:</strong> {comparison["direct"]["errors_count"]}</p>
            <p><strong>Word Count:</strong> {comparison["direct"]["word_count"]}</p>
        </div>
        <div style="flex: 1; padding: 10px; background: #e8f4f8; border-radius: 5px; margin-left: 10px;">
            <h3>MCO-Orchestrated AutoGPT</h3>
            <p><strong>Time:</strong> {comparison["mco"]["execution_time"]:.2f}s</p>
            <p><strong>Steps:</strong> {comparison["mco"]["steps_count"]}</p>
            <p><strong>Tangents:</strong> {comparison["mco"]["tangents_count"]}</p>
            <p><strong>Errors:</strong> {comparison["mco"]["errors_count"]}</p>
            <p><strong>Word Count:</strong> {comparison["mco"]["word_count"]}</p>
        </div>
    </div>
    <div style="padding: 15px; background: #f0f7ff; border-radius: 5px; margin-bottom: 20px;">
        <h3>Improvement Metrics</h3>
        <p><strong>Word Count Improvement:</strong> {comparison["metrics"]["word_count_ratio"]:.2f}x more content</p>
        <p><strong>Tangent Reduction:</strong> {comparison["metrics"]["tangent_reduction"]} fewer tangents</p>
        <p><strong>Error Reduction:</strong> {comparison["metrics"]["error_reduction"]} fewer errors</p>
        <p><strong>Quality Improvements:</strong> {comparison["metrics"]["quality_improvement"]} additional quality features</p>
    </div>
    """
    
    return metrics_html, direct_output, mco_output

# Create Gradio interface
with gr.Blocks(
    title="MCO Protocol: The Missing Orchestration Layer for MCP", 
    theme=gr.themes.Soft(),
    css="""
    .gradio-container { max-width: 1200px !important; }
    .header { text-align: center; margin-bottom: 30px; }
    .trifecta { background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 20px; border-radius: 10px; margin: 20px 0; }
    .toggle-container { display: flex; align-items: center; margin: 10px 0; }
    .toggle-label { margin-left: 10px; font-weight: bold; }
    .enhanced-template { background: #f0f7ff; padding: 10px; border-radius: 5px; margin: 5px 0; }
    .comparison-container { display: flex; justify-content: space-between; }
    .comparison-column { flex: 1; padding: 10px; margin: 0 5px; border-radius: 5px; }
    .direct-column { background: #f8f9fa; }
    .mco-column { background: #e8f4f8; }
    """
) as demo:
    
    gr.Markdown("""
    <div class="header">
    
    # üöÄ MCO Protocol: The Missing Orchestration Layer for MCP
    
    **Completing the Agentic Trifecta: MCP (Data) + A2P (Communication) + MCO (Orchestration)**
    
    Transform unreliable agents into structured, autonomous workflows with progressive revelation and persistent memory.
    
    </div>
    
    <div class="trifecta">
    <h3>üåü The Agentic Trifecta</h3>
    <p><strong>üìä MCP:</strong> Model Context Protocol - Connects agents to data sources and tools</p>
    <p><strong>ü§ù A2P:</strong> Agent-to-Agent Protocol - Enables seamless agent communication</p>  
    <p><strong>üéõÔ∏è MCO:</strong> Model Configuration Orchestration - Provides reliable workflow orchestration</p>
    <p><em>Together, these protocols create the foundation for truly autonomous, production-ready AI agents.</em></p>
    </div>
    """)
    
    with gr.Tabs() as tabs:
        with gr.TabItem("SNLP Configuration"):
            with gr.Row():
                with gr.Column(scale=1):
                    gr.Markdown("### üéØ Choose Your Workflow Template")
                    template_choice = gr.Dropdown(
                        choices=["research", "development", "content"],
                        value="research", 
                        label="Template",
                        info="Select a pre-built template or customize your workflow"
                    )
                    
                    load_template_btn = gr.Button("üìã Load Template", variant="secondary")
                    
                    with gr.Row():
                        edit_values_only = gr.Checkbox(label="Edit Values Only", value=False)
                        gr.Markdown("<div class='toggle-label'>Toggle to only edit data values, not structure</div>", elem_classes=["toggle-label"])
                    
                    gr.Markdown("### ‚öôÔ∏è Core Configuration (Persistent Memory)")
                    workflow_name = gr.Textbox(
                        label="Workflow Name",
                        placeholder="Enter a name for your workflow",
                        value="Research Assistant"
                    )
                    
                    goal = gr.Textbox(
                        label="Primary Goal", 
                        placeholder="What should this workflow accomplish?",
                        lines=2
                    )
                    
                    workflow_steps = gr.Textbox(
                        label="Workflow Steps (one per line)",
                        placeholder="Step 1: Research the topic\nStep 2: Analyze findings\nStep 3: Create report", 
                        lines=5
                    )
                    
                with gr.Column(scale=1):
                    gr.Markdown("### üéØ Success Criteria (Persistent Memory)")
                    success_criteria = gr.Textbox(
                        label="Success Criteria (one per line)",
                        placeholder="Information is accurate and well-sourced\nAnalysis is thorough and insightful",
                        lines=4
                    )
                    
                    target_audience = gr.Textbox(
                        label="Target Audience",
                        placeholder="Who will use this workflow?"
                    )
                    
                    developer_vision = gr.Textbox(
                        label="Developer Vision", 
                        placeholder="What's your vision for this autonomous workflow?",
                        lines=3
                    )
                    
                    gr.Markdown("### üî• Progressive Revelation")
                    gr.Markdown("""
                    **üß† Persistent Memory:** Core + Success Criteria (always available)  
                    **‚ú® Strategic Injection:** Features (33% progress) + Styles (66% progress)  
                    **üéØ Result:** Focused agents that don't get overwhelmed but have context when needed
                    """)
            
            generate_btn = gr.Button("üöÄ Generate MCO Configuration", variant="primary", size="lg")
            
            gr.Markdown("## üìÑ Generated SNLP Files (Real MCO Implementation)")
            
            with gr.Tabs():
                with gr.TabItem("mco.core"):
                    mco_core_output = gr.Code(language="javascript", label="mco.core", lines=25)
                
                with gr.TabItem("mco.sc"):
                    mco_sc_output = gr.Code(language="javascript", label="mco.sc", lines=25)
                
                with gr.TabItem("mco.features"):
                    mco_features_output = gr.Code(language="javascript", label="mco.features", lines=25)
                
                with gr.TabItem("mco.styles"):
                    mco_styles_output = gr.Code(language="javascript", label="mco.styles", lines=25)
            
            gr.Markdown("## üöÄ MCP Integration Demo")
            mcp_demo_output = gr.Code(language="markdown", label="MCP Integration Demo", lines=25)
        
        with gr.TabItem("Modal API Integration"):
            gr.Markdown("### üî• REAL Modal API Integration - BURNING $280 CREDITS")
            gr.Markdown("""
            **‚ö†Ô∏è THIS IS REAL - NO SIMULATIONS!**
            
            This demo uses REAL Modal API for LLM inference with your $280 credits. The Modal API powers the agent's thinking and response generation within the MCO orchestration framework.
            
            **JUDGES COME QUICK - TILL CREDITS LAST!** üî•üí∏
            """)
            
            modal_api_key = gr.Textbox(
                label="üîë Modal Token (REQUIRED for real inference)",
                placeholder="Enter your Modal token to BURN CREDITS",
                type="password"
            )
            
            modal_prompt = gr.Textbox(
                label="Test Prompt",
                placeholder="Enter a prompt to test Modal API integration",
                value="Research the impact of artificial intelligence on healthcare",
                lines=3
            )
            
            modal_run_btn = gr.Button("üí∏ BURN MODAL CREDITS", variant="primary")
            
            modal_result = gr.Textbox(
                label="Modal API Response",
                lines=10
            )
        
        with gr.TabItem("Reliability Comparison"):
            gr.Markdown("### üìä AutoGPT Reliability Comparison")
            gr.Markdown("""
            This demo shows how MCO improves reliability when orchestrating AutoGPT. The comparison runs the same task through:
            1. Direct AutoGPT execution (without MCO)
            2. MCO-orchestrated AutoGPT execution
            
            The results demonstrate how MCO's progressive revelation and structured orchestration improve reliability, focus, and output quality.
            """)
            
            comparison_task = gr.Textbox(
                label="Comparison Task",
                placeholder="Enter a task to compare direct vs. MCO-orchestrated execution",
                value="Research the impact of artificial intelligence on healthcare",
                lines=3
            )
            
            comparison_run_btn = gr.Button("üöÄ Run Reliability Comparison", variant="primary")
            
            comparison_metrics = gr.HTML(
                label="Comparison Metrics"
            )
            
            with gr.Tabs():
                with gr.TabItem("Direct AutoGPT Output"):
                    direct_output = gr.Markdown(
                        label="Direct AutoGPT Output"
                    )
                
                with gr.TabItem("MCO-Orchestrated Output"):
                    mco_output = gr.Markdown(
                        label="MCO-Orchestrated Output"
                    )
        
        with gr.TabItem("Progressive Revelation Visual"):
            gr.Markdown("### üé¨ Progressive Revelation Visualization")
            gr.Markdown("""
            This interactive visualization demonstrates how MCO's progressive revelation works. It shows how different SNLP files are active at different stages of the workflow, ensuring agents have the right context at the right time.
            """)
            
            gr.HTML("""
            <div style="width: 100%; height: 600px; border: 1px solid #ddd; border-radius: 10px; overflow: hidden; position: relative;">
                <div style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; display: flex; flex-direction: column;">
                    <!-- Workflow Steps -->
                    <div style="flex: 0 0 80px; background: #f8f9fa; border-bottom: 1px solid #ddd; display: flex; align-items: center; padding: 0 20px;">
                        <div style="display: flex; width: 100%; justify-content: space-between;">
                            <div class="step" style="padding: 10px; border-radius: 5px; background: #e8f4f8; font-weight: bold;">1. Understand</div>
                            <div class="step" style="padding: 10px; border-radius: 5px;">2. Research</div>
                            <div class="step" style="padding: 10px; border-radius: 5px;">3. Analyze</div>
                            <div class="step" style="padding: 10px; border-radius: 5px;">4. Synthesize</div>
                            <div class="step" style="padding: 10px; border-radius: 5px;">5. Create</div>
                            <div class="step" style="padding: 10px; border-radius: 5px;">6. Review</div>
                        </div>
                    </div>
                    
                    <!-- Main Content Area -->
                    <div style="flex: 1; display: flex;">
                        <!-- Context Panel -->
                        <div style="flex: 0 0 250px; background: #f0f7ff; border-right: 1px solid #ddd; padding: 20px; overflow-y: auto;">
                            <h3>Active Context</h3>
                            <div class="context-file" style="margin: 10px 0; padding: 10px; background: #ff6b6b; color: white; border-radius: 5px;">
                                <strong>mco.core</strong>
                                <div style="font-size: 0.8em; margin-top: 5px;">Persistent throughout workflow</div>
                            </div>
                            <div class="context-file" style="margin: 10px 0; padding: 10px; background: #cc5de8; color: white; border-radius: 5px;">
                                <strong>mco.sc</strong>
                                <div style="font-size: 0.8em; margin-top: 5px;">Persistent throughout workflow</div>
                            </div>
                            <div class="context-file" style="margin: 10px 0; padding: 10px; background: #ddd; color: #666; border-radius: 5px;">
                                <strong>mco.features</strong>
                                <div style="font-size: 0.8em; margin-top: 5px;">Injected at step 2 (33%)</div>
                            </div>
                            <div class="context-file" style="margin: 10px 0; padding: 10px; background: #ddd; color: #666; border-radius: 5px;">
                                <strong>mco.styles</strong>
                                <div style="font-size: 0.8em; margin-top: 5px;">Injected at step 5 (66%)</div>
                            </div>
                        </div>
                        
                        <!-- Agent Thought Area -->
                        <div style="flex: 1; padding: 20px; overflow-y: auto;">
                            <h3>Agent Thought Process</h3>
                            <div style="background: white; border: 1px solid #ddd; border-radius: 5px; padding: 15px; margin-top: 10px;">
                                <p><strong>Step 1: Understand</strong></p>
                                <p>I need to understand the research topic on AI Agent Orchestration and define key questions to guide my research. Based on the persistent context from mco.core and mco.sc, I'll focus on:</p>
                                <ul>
                                    <li>What are the key challenges in AI agent reliability?</li>
                                    <li>How does orchestration improve agent performance?</li>
                                    <li>What frameworks exist for agent orchestration?</li>
                                </ul>
                                <p>I'll ensure my approach aligns with the success criteria of creating a comprehensive, well-sourced research report with actionable insights.</p>
                            </div>
                        </div>
                        
                        <!-- JSON Context View -->
                        <div style="flex: 0 0 300px; background: #282c34; color: #abb2bf; border-left: 1px solid #ddd; padding: 20px; font-family: monospace; overflow-y: auto;">
                            <h3 style="color: #e5c07b;">Context JSON</h3>
                            <pre style="font-size: 0.8em;">
{
  "persistent_context": {
    "workflow": "Research Assistant",
    "topic": "AI Agent Orchestration",
    "research_questions": [
      "What are the key challenges in AI agent reliability?",
      "How does orchestration improve agent performance?",
      "What frameworks exist for agent orchestration?"
    ],
    "goal": "Create comprehensive, well-sourced research reports that provide actionable insights",
    "success_criteria": [
      "Information is accurate and from credible sources",
      "Analysis identifies clear patterns and insights",
      "Report is well-structured with executive summary",
      "Conclusions are supported by evidence",
      "Report provides actionable recommendations"
    ]
  },
  "injected_context": {},
  "current_step": "Understand",
  "progress": "16%"
}
                            </pre>
                        </div>
                    </div>
                    
                    <!-- Controls -->
                    <div style="flex: 0 0 60px; background: #f8f9fa; border-top: 1px solid #ddd; display: flex; align-items: center; justify-content: center; gap: 10px; padding: 0 20px;">
                        <button style="padding: 8px 15px; background: #4dabf7; color: white; border: none; border-radius: 5px; cursor: pointer;">‚èÆÔ∏è Reset</button>
                        <button style="padding: 8px 15px; background: #4dabf7; color: white; border: none; border-radius: 5px; cursor: pointer;">‚è™ Previous</button>
                        <button style="padding: 8px 15px; background: #4dabf7; color: white; border: none; border-radius: 5px; cursor: pointer;">‚ñ∂Ô∏è Play</button>
                        <button style="padding: 8px 15px; background: #4dabf7; color: white; border: none; border-radius: 5px; cursor: pointer;">‚è© Next</button>
                    </div>
                </div>
            </div>
            """)
            
            gr.Markdown("""
            ### üîç How Progressive Revelation Works
            
            1. **Persistent Context**: mco.core and mco.sc files remain in the agent's context throughout the entire workflow
            2. **Strategic Injection**: mco.features is injected during implementation steps (around 33% progress)
            3. **Final Formatting**: mco.styles is injected during formatting steps (around 66% progress)
            
            This approach prevents cognitive overload while ensuring the agent has all necessary information at the right time.
            """)
    
    # Event handlers
    load_template_btn.click(
        load_template_data,
        inputs=[template_choice],
        outputs=[workflow_name, goal, workflow_steps, success_criteria, target_audience, developer_vision]
    )
    
    generate_btn.click(
        generate_all_files,
        inputs=[template_choice, workflow_name, goal, workflow_steps, success_criteria, target_audience, developer_vision, edit_values_only],
        outputs=[mco_core_output, mco_sc_output, mco_features_output, mco_styles_output, mcp_demo_output]
    )
    
    modal_run_btn.click(
        run_modal_integration,
        inputs=[modal_api_key, modal_prompt],
        outputs=[modal_result]
    )
    
    comparison_run_btn.click(
        run_autogpt_comparison_demo,
        inputs=[comparison_task],
        outputs=[comparison_metrics, direct_output, mco_output]
    )
    
    # Auto-load research template on startup
    demo.load(
        load_template_data,
        inputs=[gr.State("research")],
        outputs=[workflow_name, goal, workflow_steps, success_criteria, target_audience, developer_vision]
    )

if __name__ == "__main__":
    demo.launch(share=True, debug=True)
